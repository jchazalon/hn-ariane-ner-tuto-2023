{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse 1 : utilisation de regexp et de patterns spacy\n",
    "\n",
    "on essaie d'extraire des termes connus, et des locutions typiques, pour d√©tecter les r√©f√©rences aux personnages et aux lieux.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trouver et lister les fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME fournir code lecture fichiers depuis Google Drive ici (archive format zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TEXT_FILES_DIR = \"/home/jchazalo/tmp/French_ELTEC_NER_Open_Dataset/texts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/jchazalo/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00101_Adam.txt',\n",
       " '/home/jchazalo/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00102_Adam.txt',\n",
       " '/home/jchazalo/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00201_Audoux.txt',\n",
       " '/home/jchazalo/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00301_Aimard.txt',\n",
       " '/home/jchazalo/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00302_Aimard.txt',\n",
       " '/home/jchazalo/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00401_Allais.txt',\n",
       " '/home/jchazalo/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00501_Balzac.txt',\n",
       " '/home/jchazalo/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00502_Balzac.txt',\n",
       " '/home/jchazalo/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00503_Balzac.txt',\n",
       " '/home/jchazalo/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00601_Boisgobey.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = sorted(glob(os.path.join(PATH_TEXT_FILES_DIR, \"*.txt\")))\n",
    "print(\"Found\", len(files), \"files.\")\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jchazalo/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA03001_Ohnet.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = files[50]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import spacy and start processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## L'objet [`Language`](https://spacy.io/api/language)\n",
    "On construit une nouvelle cha√Æne de traitements de plusieurs fa√ßon. La mani√®re la plus simple est de construire une cha√Æne de traitement vide (ou presque) pour le fran√ßais √† l'aide de la \"fabrique\" √† cha√Ænes de traitement `spacy.blank(LANGAGE)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La cha√Æne de traitements contient diff√©rents traitements appliqu√©s les uns apr√®s les autres.\n",
    "On peut afficher cette liste de traitements √† l'aide de l'attribut `pipe_names` de l'objet `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par d√©faut, une cha√Æne de traitement ne contient rien‚Ä¶ Sauf un *tokenizer*, d'o√π l'importance de pr√©ciser la langue !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'objet [`Doc`](https://spacy.io/api/doc)\n",
    "On obtient un objet [`Doc`](https://spacy.io/api/doc) en appliquant la cha√Æne de traitement [`Language`](https://spacy.io/api/language) √† une cha√Æne de texte.\n",
    "\n",
    "Cet objet [`Doc`](https://spacy.io/api/doc) est central pour Spacy car va √™tre progressivement enrichi par chacun de traitements qui va venir y piocher les informations dont il a besoin en entr√©e, et ajouter les informations qu'il a calcul√©es.\n",
    "Par exemple, le composant \"ner\" va venir affecter une √©tiquette (*\"label\"*) √† chacun des *tokens* du document. Il va stocker cette information dans un nouvel attribut `doc.ents` du document.\n",
    "\n",
    "L'attribut `doc.text` quant √† lui contient la liste des *tokens* extraits.\n",
    "\n",
    "![](https://spacy.io/images/architecture.svg)\n",
    "\n",
    "![](https://spacy.io/images/pipeline.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©√© en traitant une chaine de caract√®res avec l'objet nlp\n",
    "doc = nlp(\"Bonjour tout le monde !\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut parcourir les *tokens* extraits d'un [`Doc`](https://spacy.io/api/doc) √† l'aide d'une boucle classique en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour\n",
      "tout\n",
      "le\n",
      "monde\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# It√®re sur les tokens dans un Doc\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tout"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On peut s√©lectionner un token particulier, gr√¢ce √† son indice dans le document\n",
    "token = doc[1]\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de cellule avec contenu cach√© par d√©faut\n",
    "#### üöß <b>Essayez √† pr√©sent de s√©lectionner et afficher les <i>tokens</i> \"tout le monde\".</b>\n",
    "\n",
    "<details>\n",
    "<summary>Indices</summary>\n",
    "\n",
    "Vous pouvez utiliser les *ranges* pour s√©lectionner plusieurs √©l√©ments d'un it√©rable. Voici un exemple de la syntaxe √† utiliser :\n",
    "```python\n",
    "ma_liste = [0, 1, 2, 3]\n",
    "print(ma_liste[1:3])\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "On applique cette syntaxe pour s√©lectionner les tokens du rang 1 (2e token, inclus) au rang 4 (non inclus) :\n",
    "\n",
    "```python\n",
    "doc[1:4]\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tout le monde"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On peut √©galement utiliser les \"ranges\" Python pour s√©lectionner plusieurs tokens\n",
    "span = doc[1:4]\n",
    "span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tout le monde'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On peut √©galement acc√©der aux attributs d'un span\n",
    "span.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index :    [0, 1, 2, 3, 4]\n",
      "Text :     ['Cela', 'co√ªte', '5', '‚Ç¨', '.']\n",
      "is_alpha : [True, True, False, False, False]\n",
      "is_punct : [False, False, False, False, True]\n",
      "like_num : [False, False, True, False, False]\n"
     ]
    }
   ],
   "source": [
    "## Autres attributs des tokens et des spans\n",
    "\n",
    "doc = nlp(\"Cela co√ªte 5 ‚Ç¨.\")\n",
    "\n",
    "print(\"Index :   \", [token.i for token in doc])\n",
    "print(\"Text :    \", [token.text for token in doc])\n",
    "\n",
    "print(\"is_alpha :\", [token.is_alpha for token in doc])\n",
    "print(\"is_punct :\", [token.is_punct for token in doc])\n",
    "print(\"like_num :\", [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation d'un pipeline avec un pos_tagger et un reconnaisseur d'entit√©s nomm√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from fr-core-news-sm==3.7.0) (3.7.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.3.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.1.3)\n",
      "Installing collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "# Charge un vocabulaire fran√ßais (utilis√© pour la tokenization), et bien d'autres composants !\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        La    DET          det    journ√©e\n",
      "   journ√©e   NOUN        nsubj    d√©roule\n",
      "        de    ADP         case  formation\n",
      " formation   NOUN         nmod    journ√©e\n",
      "         √†    ADP         case       Lyon\n",
      "      Lyon  PROPN         nmod    journ√©e\n",
      "        se   PRON    expl:comp    d√©roule\n",
      "   d√©roule   VERB         ROOT    d√©roule\n",
      "      bien    ADV       advmod    d√©roule\n",
      "         .  PUNCT        punct    d√©roule\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"La journ√©e de formation √† Lyon se d√©roule bien.\")\n",
    "for token in doc:\n",
    "    # print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "    print(f\"{token.text:>10s}\", f\"{token.pos_:>6s}\", f\"{token.dep_:>12s}\", f\"{token.head.text:>10s}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des r√©sultats avec *displaCy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">La journ√©e de formation √† \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lyon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " se d√©roule bien.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"spans\" style=\"line-height: 2.5; direction: ltr\">La journ√©e de formation √† Lyon se d√©roule bien . </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"span\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"fr\" id=\"7e8685702ac54800838047503236bf93-0\" class=\"displacy\" width=\"1625\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">La</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">journ√©e</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">de</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">formation</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">√†</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Lyon</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">se</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">d√©roule</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">bien.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7e8685702ac54800838047503236bf93-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7e8685702ac54800838047503236bf93-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7e8685702ac54800838047503236bf93-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,2.0 1275.0,2.0 1275.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7e8685702ac54800838047503236bf93-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7e8685702ac54800838047503236bf93-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7e8685702ac54800838047503236bf93-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7e8685702ac54800838047503236bf93-0-3\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7e8685702ac54800838047503236bf93-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,354.0 L573.0,342.0 557.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7e8685702ac54800838047503236bf93-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7e8685702ac54800838047503236bf93-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7e8685702ac54800838047503236bf93-0-5\" stroke-width=\"2px\" d=\"M245,352.0 C245,89.5 920.0,89.5 920.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7e8685702ac54800838047503236bf93-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,354.0 L928.0,342.0 912.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7e8685702ac54800838047503236bf93-0-6\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7e8685702ac54800838047503236bf93-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">expl:comp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7e8685702ac54800838047503236bf93-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7e8685702ac54800838047503236bf93-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1435.0,354.0 L1443.0,342.0 1427.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation de la nouvelle cha√Æne de traitement pour traiter nos donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(filename: str) -> str:\n",
    "    with open(filename, encoding=\"utf8\") as in_file:\n",
    "        return \"\".join(in_file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "Dans un de ces charmants chemins creux de Normandie, serpentant entre les lev√©es, plant√©es de grands arbres, qui entourent les fermes d'un rempart de verdure imp√©n√©trable au vent et au soleil, par une belle matin√©e d'√©t√©, une amazone, mont√©e sur une jument de forme assez m√©diocre, s'avan√ßait au pas, les r√™nes abandonn√©es, r√™veuse, respirant l'air ti√®de, embaum√© du parfum des tr√®fles en fleurs. Avec son chapeau de feutre noir entour√© d'un voile de gaze blanche, son costume de drap gris fer √† longue jupe, elle avait fi√®re tournure. On e√ªt dit une de ces aventureuses grandes dames qui, au temps de Stofflet et de Cathelineau, suivaient hardiment l'arm√©e royaliste, dans les tra√Ænes du Bocage, et √©clairaient de leur sourire la sombre √©pop√©e vend√©enne.\n",
      "√âl√©gante et svelte, elle se laissait aller gracieusement au mouvement de sa monture, fouettant distraitement de sa cravache les tiges vertes des gen√™ts. Un l√©vrier d'√âcosse au poil rude et rouge√¢tre l'accompagnait, r√©glant son allure souple sur la marche lass√©e du cheval, et levant, de temps en temps, vers sa ma√Ætresse, sa t√™te pointue, √©clair√©e par deux yeux noirs qui brillaient sous des sourcils en broussailles. L'herbe courte et grasse, qui poussait sous la vo√ªte sombre des h√™tres, √©tendait devant la promeneuse un tapis moelleux comme du velours. Dans les herbages, les vaches appesanties tendaient vers la fra√Æcheur du chemin leurs mufles tourment√©s par les mouches. Pas un souffle de vent n'agitait les feuilles. Sous les feux du soleil l'air vibrait embras√©, et une torpeur lourde pesait sur la terre.\n",
      "La t√™te pench√©e sur la poitrine, absorb√©e, l'amazone allait, indiff√©rente au charme de ce chemin plein d'ombre et de silence.\n",
      "Soudainement, son cheval fit un √©cart, pointa les oreilles, et faillit se renverser, soufflant bruyamment, tandis que le l√©vrier, s'√©lan√ßant en avant, aboyait avec fureur, et montrait √† un homme qui venait de sauter dans le chemin creux une double rang√©e de dents aigu√´s et grin√ßantes.\n",
      "L'amazone, tir√©e brutalement de sa m√©ditation, rassembla les r√™nes, ramena son cheval et, s'assurant sur sa selle, adressa √† l'auteur de tout ce trouble un regard plus √©tonn√© que m√©content.\n",
      "‚Äî Je vous demande bien pardon, Madame, dit celui-ci d'une voix pleine et sonore... Je me suis tr√®s maladroitement √©lanc√© en travers de votre route... Je ne vous entendais pas arriver... Il y a plus d'une heure que je tourne dans ces herbages sans pouvoir en sortir... Toutes les barri√®res des cours sont cadenass√©es, et les haies sont trop hautes pour qu'on puisse les franchir... Enfin j'ai trouv√© ce petit chemin cach√© sous les arbres, et, en y prenant pied, j'ai failli vous faire jeter √† terre...\n",
      "L'amazone sourit un peu, et son visage aux traits nobles et d√©licats prit une expression enjou√©e et charmante :\n",
      "‚Äî Rassurez-vous, Monsieur : vous n'√™tes pas tr√®s coupable, et je ne tombe pas de cheval si facilement que vous paraissez le croire...\n",
      "Et comme son l√©vrier continuait √† gronder en mena√ßant:\n",
      "‚Äî Allons, Fox, la paix ! dit-elle.\n",
      "Le chien se retourna et, se m√¢tant sur ses pattes de derri√®re, posa son museau fin sur la main de sa ma√Ætresse. Celle-ci, tout en caressant le l√©vrier, examinait son interlocuteur. C'√©tait un homme d'une trentaine d'ann√©es, de haute taille, au visage √©nergique, encadr√© d'une √©paisse barbe brune. Sa l√®vre ras√©e et son teint basan√© lui donnaient l'air d'un marin. Il √©tait v√™tu d'un costume complet de drap chin√©, coiff√© d'un chapeau de feutre mou, et √† la main il tenait une canne en bois de fer, mieux faite pour la bataille que pour la promenade.\n",
      "‚Äî Vous n'√™tes pas de ce pays ? demanda alors l'amazone.\n",
      "‚Äî Je suis ici seulement depuis hier, dit l'√©tranger, sans r√©pondre √† la question qui lui √©tait pos√©e... J'ai eu la fantaisie d'aller me promener ce matin dans la campagne, et je me suis √©gar√©... J'ai pourtant l'habitude de m'orienter... Mais ces diables de petits chemins qui n'aboutissent √† rien forment un labyrinthe inextricable...\n",
      "‚Äî O√π d√©sirez-vous aller ?\n",
      "‚Äî √Ä La Neuville...\n",
      "‚Äî Tr√®s bien ! Vous lui tournez le dos... Si vous voulez me suivre pendant quelques instants, je vous mettrai dans une route o√π vous ne risquerez plus de vous perdre...\n",
      "‚Äî Bien volontiers, Madame... Mais j'esp√®re que vous ne vous √©loignerez pas de la direction que vous suiviez...\n",
      "L'amazone secoua gravement la t√™te, et dit:\n",
      "‚Äî Cela ne me d√©tourne point d'un seul pas... L'√©tranger fit un signe d'acquiescement, et, s√©par√© de la jeune femme par le l√©vrier, qui ne revenait pas de son antipathie et trottait en grondant sourdement, il suivit la fra√Æche et verte perc√©e, ne parlant pas, mais admirant la beaut√© rayonnante de son guide. Par moments, des branches basses, pendant des troncs d'arbres, barraient le chemin, et l'amazone √©tait oblig√©e de courber la t√™te pour les √©viter. Dans ce mouvement, sous son feutre, apparaissait sa nuque blanche sur laquelle frisaient des m√®ches folles, et son pur profil se d√©tachait sur le fond sombre de la verdure. Elle se penchait souple et se redressait avec une gr√¢ce √©l√©gante et simple, ne paraissant pas se douter qu'elle √©tait admir√©e, et, soit par fiert√©, soit par insouciance, ne tenant aucun compte du compagnon que le hasard lui avait donn√©. Au repos, son visage exprimait une gravit√© m√©lancolique, comme si elle vivait sous l'empire d'une habituelle tristesse. Quels chagrins pouvait avoir cette jeune et belle personne cr√©√©e pour √™tre servie, choy√©e et ador√©e ? La destin√©e injuste lui avait-elle donn√© le malheur, √† elle faite pour la joie ? Elle semblait riche. Sa peine devait donc √™tre toute morale. Arriv√© √† ce point de ses inductions, l'√©tranger se demanda si sa compagne √©tait une jeune femme ou une jeune fille. Sa haute taille, ses √©paules rondes, dont l'harmonieuse ampleur √©tait accentu√©e par la finesse de sa ceinture, √©taient d'une femme. Mais la suavit√© velout√©e de ses joues, la fra√Æche puret√© de ses yeux trahissaient la jeune fille. Le lobe ros√© de ses oreilles n'√©tait point perc√©, et ni au cou ni aux poignets elle ne portait de bijou.\n"
     ]
    }
   ],
   "source": [
    "text = load_text(files[50])\n",
    "print(text)\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I LOC\n",
      "Normandie LOC\n",
      "Stofflet PER\n",
      "Cathelineau LOC\n",
      "Bocage LOC\n",
      "√âl√©gante ORG\n",
      "√âcosse LOC\n",
      "amazone LOC\n",
      "Madame PER\n",
      "Rassurez PER\n",
      "Monsieur PER\n",
      "Allons PER\n",
      "Fox ORG\n",
      "O√π MISC\n",
      "La Neuville LOC\n",
      "Madame PER\n"
     ]
    }
   ],
   "source": [
    "# It√®re sur les entit√©s pr√©dites\n",
    "for ent in doc.ents:\n",
    "    # Affiche le texte de l'entit√© et son label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Non-GPE locations, mountain ranges, bodies of water'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"LOC\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (‚ùì opt ‚ùì) Utilisation du matcher Spacy\n",
    "TODO montrer comment filtrer sur lex√®me, nature ou fonction du token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"POS\": \"PROPN\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"PROPN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"NOMS_PROPRES\", [pattern])\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de correspondances trouv√©es : 10\n",
      "Correspondance trouv√©e : Normandie\n",
      "Correspondance trouv√©e : Stofflet\n",
      "Correspondance trouv√©e : Cathelineau\n",
      "Correspondance trouv√©e : √âcosse\n",
      "Correspondance trouv√©e : Rassurez\n",
      "Correspondance trouv√©e : Allons\n",
      "Correspondance trouv√©e : Fox\n",
      "Correspondance trouv√©e : Neuville\n",
      "Correspondance trouv√©e : volontiers\n",
      "Correspondance trouv√©e : hasard\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre de correspondances trouv√©es :\", len(matches))\n",
    "\n",
    "# It√®re sur les correspondances et affiche la portion de texte\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Correspondance trouv√©e :\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement massif des donn√©es\n",
    "\n",
    "TODO documentation de l'utilisation de `nlp.pipe` plut√¥t √† ce moment-l√† ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsqu'on souhaite traiter plusieurs documents, on peut utiliser `nlp.pipe(LISTE_DE_TEXTES)`.\n",
    "Dans ce cas, on obtient une liste de documents en sortie, qu'il est possible d'inspecter avec une seconde boucle !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc#0, tok#0: Bonjour\n",
      "doc#0, tok#1: tout\n",
      "doc#0, tok#2: le\n",
      "doc#0, tok#3: monde\n",
      "doc#0, tok#4: !\n",
      "doc#1, tok#0: Comment\n",
      "doc#1, tok#1: allez\n",
      "doc#1, tok#2: -vous\n",
      "doc#1, tok#3: ?\n",
      "doc#1, tok#4: Bien\n",
      "doc#1, tok#5: ,\n",
      "doc#1, tok#6: j'\n",
      "doc#1, tok#7: esp√®re\n",
      "doc#1, tok#8: !\n",
      "doc#2, tok#0: Savez\n",
      "doc#2, tok#1: -vous\n",
      "doc#2, tok#2: qu'\n",
      "doc#2, tok#3: une\n",
      "doc#2, tok#4: cha√Æne\n",
      "doc#2, tok#5: de\n",
      "doc#2, tok#6: caract√®res\n",
      "doc#2, tok#7: peut\n",
      "doc#2, tok#8: contenir\n",
      "doc#2, tok#9: des\n",
      "doc#2, tok#10: retours\n",
      "doc#2, tok#11: √†\n",
      "doc#2, tok#12: la\n",
      "doc#2, tok#13: ligne\n",
      "doc#2, tok#14: \n",
      "\n",
      "doc#2, tok#15: comme\n",
      "doc#2, tok#16: celui-ci\n",
      "doc#2, tok#17: ?\n"
     ]
    }
   ],
   "source": [
    "TEXTES = [\n",
    "    \"Bonjour tout le monde !\", \n",
    "    \"Comment allez-vous ? Bien, j'esp√®re !\",\n",
    "    \"Savez-vous qu'une cha√Æne de caract√®res peut contenir des retours √† la ligne\\ncomme celui-ci ?\"\n",
    "    ]\n",
    "documents = nlp.pipe(TEXTES)\n",
    "for doc_id, doc in enumerate(documents):\n",
    "    for token in doc:\n",
    "        print(f\"doc#{doc_id}, tok#{token.i}: {token.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ¬µs, sys: 0 ns, total: 4 ¬µs\n",
      "Wall time: 7.15 ¬µs\n",
      "Got 1655 entites about persons, and 1562 entities about spatial objects.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# FIXME use the dataset file here\n",
    "documents = nlp.pipe([load_text(path) for path in files])\n",
    "per_tokens = []\n",
    "spat_tokens = []\n",
    "for doc in documents:\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in (\"PER\"):\n",
    "            per_tokens.append(ent)\n",
    "        elif ent.label_ in (\"LOC\", \"GPE\"):\n",
    "            spat_tokens.append(ent)\n",
    "print(f\"Got {len(per_tokens)} entites about persons, and {len(spat_tokens)} entities about spatial objects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I,\n",
       " Norine Duclos,\n",
       " Adonc,\n",
       " Rose,\n",
       " Norine,\n",
       " Saint-Brunelle,\n",
       " Armandine,\n",
       " Rose,\n",
       " r√©serve de la Rose,\n",
       " Norine,\n",
       " couseuses,\n",
       " couseuses,\n",
       " Garde-les,\n",
       " Tiens,\n",
       " Norine,\n",
       " Bassette,\n",
       " bras-le-corps,\n",
       " Sorgues,\n",
       " Vaucluse,\n",
       " Ch√¢teau de Saint-Est√®ve,\n",
       " Durance,\n",
       " notre Provence,\n",
       " Oasis,\n",
       " Saint-Est√®ve,\n",
       " Durance,\n",
       " Notre voisine,\n",
       " Paris,\n",
       " Prologue,\n",
       " l'Am√©rique,\n",
       " l'Am√©rique,\n",
       " Am√©rique,\n",
       " Espagnols,\n",
       " les Incas,\n",
       " l'Europe,\n",
       " l'Am√©rique,\n",
       " √âtats,\n",
       " √âtat de Sonora,\n",
       " rio Gila,\n",
       " √âtat,\n",
       " sierra Madre,\n",
       " golfe de Californie,\n",
       " sierra Madre,\n",
       " Durango,\n",
       " √âtats de Durango,\n",
       " Guadalajara,\n",
       " Pacifique,\n",
       " Sonora,\n",
       " rio Gila,\n",
       " sierra Madre,\n",
       " Indiens,\n",
       " Comanches,\n",
       " Pawnees,\n",
       " Pimas,\n",
       " Opatas,\n",
       " Apaches,\n",
       " Sonora,\n",
       " Guaymas,\n",
       " Hermosillo,\n",
       " Pacifique,\n",
       " Montagne,\n",
       " Hermosillo,\n",
       " Gambusinos,\n",
       " Rosario,\n",
       " rue du Rosaire,\n",
       " Mexique,\n",
       " I,\n",
       " l'Am√©rique,\n",
       " la France,\n",
       " Canada,\n",
       " Nouvelle-France,\n",
       " Anglais,\n",
       " Angleterre,\n",
       " Mississipiens,\n",
       " la France,\n",
       " Canada,\n",
       " Europ√©ens,\n",
       " Indiens,\n",
       " Nouvelle-France,\n",
       " baie d'Hudson,\n",
       " golfe du Mexique,\n",
       " Nouvelle-Orl√©ans,\n",
       " la France,\n",
       " Canada,\n",
       " Ville de Paris\n",
       " Aux √âlecteurs du IXe arrondissement,\n",
       " h√¥tel Saint-P√©tersbourg,\n",
       " rue Royale,\n",
       " Silver-Grill,\n",
       " Astorg,\n",
       " avenue d'Eylau,\n",
       " Bureaucratie,\n",
       " Europe,\n",
       " Cap,\n",
       " Qu√©bec,\n",
       " Cap,\n",
       " Observatoire,\n",
       " Labrador,\n",
       " Cap,\n",
       " de Besan√ßon,\n",
       " Watteville,\n",
       " de Besan√ßon,\n",
       " Comt√©,\n",
       " Rupt,\n",
       " Watteville,\n",
       " Watteville,\n",
       " Suisse,\n",
       " Rupt,\n",
       " Pr√©fecture,\n",
       " Rupt,\n",
       " rue du Perron,\n",
       " de Besan√ßon,\n",
       " Italie,\n",
       " Watteville,\n",
       " Rupt,\n",
       " de Besan√ßon,\n",
       " h√¥tel de Rupt,\n",
       " damas,\n",
       " de Saxe,\n",
       " Paris,\n",
       " Besan√ßon,\n",
       " Watteville,\n",
       " Besan√ßon,\n",
       " Rupt,\n",
       " Paris,\n",
       " Conseillers,\n",
       " Chapitre,\n",
       " Droit,\n",
       " Politique,\n",
       " Bisontins,\n",
       " Chapitre,\n",
       " cath√©drale de Besan√ßon,\n",
       " Paris,\n",
       " rue du Perron,\n",
       " Comt√©,\n",
       " Italiens,\n",
       " Paris,\n",
       " Paris,\n",
       " Parisien,\n",
       " Paris,\n",
       " l'Empire,\n",
       " Anglais,\n",
       " France,\n",
       " l'Empire,\n",
       " L'Empire,\n",
       " Chinois,\n",
       " Fran√ßais,\n",
       " ch√¢le,\n",
       " Garat,\n",
       " Beau,\n",
       " Incroyables,\n",
       " Paris,\n",
       " Fran√ßais,\n",
       " Paris,\n",
       " Institut,\n",
       " Combien,\n",
       " l'Empire,\n",
       " France,\n",
       " Beaux-Arts,\n",
       " √âgyptiens,\n",
       " la France,\n",
       " Decamps,\n",
       " √âtat,\n",
       " Rome,\n",
       " Pens√©e,\n",
       " Bric-√†-Brac,\n",
       " Euterpe,\n",
       " Paris,\n",
       " Rome,\n",
       " Italie,\n",
       " Rome,\n",
       " Venise,\n",
       " Milan,\n",
       " Florence,\n",
       " Bologne,\n",
       " Naples,\n",
       " France,\n",
       " Paris,\n",
       " T√©riaskis,\n",
       " Paris,\n",
       " Contrepoint,\n",
       " Rossini,\n",
       " Rome,\n",
       " Paris,\n",
       " Rome,\n",
       " Paris,\n",
       " S√®vres,\n",
       " Auvergnats,\n",
       " Bande-Noire,\n",
       " la France-Pompadour,\n",
       " Lepautre,\n",
       " Lavall√©e-Poussin,\n",
       " Bricabraquologie,\n",
       " mus√©e Pons,\n",
       " Main,\n",
       " rue des Je√ªneurs,\n",
       " Bric-√†-Brac,\n",
       " l'Empire,\n",
       " la France,\n",
       " √âliante,\n",
       " l'Empire,\n",
       " Paris,\n",
       " La Table,\n",
       " Paris,\n",
       " Recette,\n",
       " Habitu√©,\n",
       " Paris,\n",
       " Timidit√©,\n",
       " Porto,\n",
       " V√©nus,\n",
       " Italie,\n",
       " Bric-√†-Brac,\n",
       " Bas-Limoges,\n",
       " Vieille-Poste,\n",
       " rue de la Cit√©,\n",
       " B√¢tis,\n",
       " rue de la Cit√©,\n",
       " Vieille-Poste,\n",
       " Limousins,\n",
       " Sauviat,\n",
       " Auvergne,\n",
       " Paris,\n",
       " Bande Noire,\n",
       " Sauviat,\n",
       " Limoges,\n",
       " Limoges,\n",
       " Sauviat,\n",
       " vieux Champagnac,\n",
       " Auvergne,\n",
       " Sauviat,\n",
       " Champagnac,\n",
       " Harpagon,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Vieille-Poste,\n",
       " rue de la Cit√©,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Paris,\n",
       " Paris,\n",
       " Paris,\n",
       " Bande Noire,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Saint-√âtienne,\n",
       " Sauviat,\n",
       " Vierge,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Vienne,\n",
       " Saint-√âtienne,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Paris,\n",
       " rue de la Cit√©,\n",
       " Sauviat,\n",
       " damas rouge,\n",
       " Mieris,\n",
       " Terburg,\n",
       " Sauviat,\n",
       " Auvergne,\n",
       " Virginie,\n",
       " Ne ferais-tu pas bien de le montrer,\n",
       " Saint-√âtienne,\n",
       " G√©nie,\n",
       " Virginie,\n",
       " Id√©al,\n",
       " Vienne,\n",
       " Limoges,\n",
       " Ile-de-France,\n",
       " Vienne,\n",
       " Paris,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Paris,\n",
       " V√©ronique,\n",
       " Sauviat,\n",
       " V√©ronique,\n",
       " Sauviat,\n",
       " Limoges,\n",
       " Sauviat,\n",
       " Auvergne,\n",
       " D√©partement,\n",
       " place des Arbres,\n",
       " Limoges,\n",
       " rue Montantmanigne,\n",
       " Sauviat,\n",
       " Receveur-g√©n√©ral,\n",
       " Pr√©fecture,\n",
       " Conseil-g√©n√©ral du D√©partement,\n",
       " Limoges,\n",
       " Limoges,\n",
       " Limousin,\n",
       " Sauviat,\n",
       " Limoges,\n",
       " Sauviat,\n",
       " Auvergne,\n",
       " Bourges,\n",
       " ‚Äî Est,\n",
       " Paris,\n",
       " Sauviat,\n",
       " Paris,\n",
       " Limousin,\n",
       " Limoges,\n",
       " Sauviat,\n",
       " Paris,\n",
       " Limoges,\n",
       " h√¥tel Graslin,\n",
       " Paris,\n",
       " Limousin,\n",
       " Limoges,\n",
       " Sauviat,\n",
       " Sauviat,\n",
       " Paris,\n",
       " boulevard Saint-Germain,\n",
       " rue du Cardinal-Lemoine,\n",
       " Halle,\n",
       " place Pigalle,\n",
       " Jardin des Plantes,\n",
       " quai Saint-Bernard,\n",
       " Paris,\n",
       " boulevard Montmartre,\n",
       " Dinan,\n",
       " Panoramas,\n",
       " rue d'Enghien,\n",
       " d√©partement du Cher,\n",
       " Cher,\n",
       " I,\n",
       " rue Saint-Honor√©,\n",
       " Saint-Gabriel,\n",
       " Lameth,\n",
       " Paris,\n",
       " Bussy,\n",
       " Que veux-tu dire,\n",
       " Sissone,\n",
       " Bussy-Lameth,\n",
       " comtesse de Lameth,\n",
       " Paris,\n",
       " Picardie,\n",
       " Roucy,\n",
       " I,\n",
       " Languedoc,\n",
       " Midi,\n",
       " Eyssette,\n",
       " Annou,\n",
       " Marseille,\n",
       " Marseille,\n",
       " Marseille,\n",
       " Annou,\n",
       " Eyssette,\n",
       " Annou,\n",
       " Eyssette,\n",
       " Eyssette,\n",
       " Annou,\n",
       " Annou,\n",
       " Oc√©an,\n",
       " Toutoune,\n",
       " Toutoune,\n",
       " Paris,\n",
       " Serquigny,\n",
       " I,\n",
       " Lazare,\n",
       " Paris,\n",
       " Mo√´t,\n",
       " sabl√©,\n",
       " Moselle,\n",
       " Tockay,\n",
       " Nouvelle-Orl√©ans,\n",
       " D√©cid√©ment,\n",
       " I,\n",
       " nacre,\n",
       " Seille,\n",
       " Sapho,\n",
       " Sapho,\n",
       " Fran√ßais,\n",
       " Espagnols,\n",
       " Paris,\n",
       " Florence,\n",
       " Turin,\n",
       " d'Espagne,\n",
       " Castillane,\n",
       " Italien,\n",
       " cath√©drale de Plaisance,\n",
       " √âtat,\n",
       " Su√®de,\n",
       " l'Italie,\n",
       " Allemagne,\n",
       " Turquie,\n",
       " Russie,\n",
       " la France,\n",
       " Angleterre,\n",
       " France,\n",
       " Angleterre,\n",
       " l'Europe,\n",
       " Angleterre,\n",
       " France,\n",
       " la France,\n",
       " province de Bretagne,\n",
       " Angleterre,\n",
       " √âcosse,\n",
       " Irlande,\n",
       " Ursins,\n",
       " Madrid,\n",
       " Ardennes,\n",
       " la Meuse,\n",
       " Hollande,\n",
       " Loire,\n",
       " Rh√¥ne,\n",
       " Seine,\n",
       " la France,\n",
       " France,\n",
       " comt√© de Champagne,\n",
       " for√™t des Ardennes,\n",
       " Francheval,\n",
       " Ch√¢teau-,\n",
       " la Roche,\n",
       " Sedan,\n",
       " Meuse,\n",
       " Sedan,\n",
       " Sedan,\n",
       " Sedan,\n",
       " for√™t des Ardennes,\n",
       " Luxembourg,\n",
       " comt√© de Chiny,\n",
       " Orval,\n",
       " Orval,\n",
       " Sedan,\n",
       " Raison,\n",
       " √ätre supr√™me,\n",
       " Mlle,\n",
       " Telle,\n",
       " Sedan,\n",
       " XVe,\n",
       " Mlle,\n",
       " Mesnard,\n",
       " Mesnarde,\n",
       " abbaye d'Orval,\n",
       " Douzy,\n",
       " Bazeille,\n",
       " I,\n",
       " Paris,\n",
       " Brest,\n",
       " pauvre Bretagne,\n",
       " Perche,\n",
       " Beauce,\n",
       " Orne,\n",
       " Basse-Normandie,\n",
       " Vitr√©,\n",
       " la Vilaine,\n",
       " Rennes,\n",
       " for√™t de Rennes,\n",
       " Ch√¢teaubriant,\n",
       " Laval,\n",
       " taillis,\n",
       " for√™t de Rennes,\n",
       " tonneliers,\n",
       " Bretagne,\n",
       " √âtats,\n",
       " Rennes,\n",
       " la Bretagne,\n",
       " France,\n",
       " √âtats de forfaiture,\n",
       " Bretagne,\n",
       " Pontcallec,\n",
       " Bouffay de Nantes,\n",
       " Ville-de-Montereau,\n",
       " quai Saint-Bernard,\n",
       " √Æle Saint-Louis,\n",
       " Cit√©,\n",
       " Notre-Dame,\n",
       " Paris,\n",
       " Nogent-sur-Seine,\n",
       " au Havre,\n",
       " de la Seine,\n",
       " de Russie,\n",
       " boulevard Montmartre,\n",
       " I,\n",
       " √âtude,\n",
       " Rest√©,\n",
       " Levez,\n",
       " Levez,\n",
       " Festin,\n",
       " M√©gara,\n",
       " Carthage,\n",
       " Sicile,\n",
       " Ligures,\n",
       " Lusitaniens,\n",
       " Bal√©ares,\n",
       " N√®gres,\n",
       " Rome,\n",
       " Cariens,\n",
       " Cappadoce,\n",
       " Lydiens,\n",
       " vermillon,\n",
       " Tamrapanni,\n",
       " Gaulois,\n",
       " N√®gres,\n",
       " Brutium,\n",
       " Campanie,\n",
       " Carthage,\n",
       " Rome,\n",
       " Carthage,\n",
       " Carthage,\n",
       " Lusitanien,\n",
       " Lac√©d√©moniens,\n",
       " I,\n",
       " Marraine,\n",
       " I,\n",
       " Louis-le-Grand,\n",
       " Chine,\n",
       " Souviens-toi,\n",
       " Continuerai,\n",
       " Julie,\n",
       " Paris,\n",
       " Enghien,\n",
       " Enghien,\n",
       " Enghien,\n",
       " Enghien,\n",
       " I,\n",
       " √ätes,\n",
       " Paris,\n",
       " Paris,\n",
       " place Vend√¥me,\n",
       " Allez,\n",
       " rue Castiglione,\n",
       " Vienne,\n",
       " Paris,\n",
       " Exquis,\n",
       " Barnabites,\n",
       " Palais,\n",
       " √âp√Ætre,\n",
       " Thionville,\n",
       " citoyen Dupont a√Æn√©,\n",
       " Dupont,\n",
       " √âvangile,\n",
       " Salut,\n",
       " Opticien,\n",
       " Orf√®vres,\n",
       " Mayence,\n",
       " Valenciennes,\n",
       " Fontenay,\n",
       " Vend√©ens,\n",
       " Lyon,\n",
       " C√©vennes,\n",
       " Espagnols,\n",
       " Paris,\n",
       " I,\n",
       " Osaka,\n",
       " Palais,\n",
       " Arriv√©s,\n",
       " lune,\n",
       " conduis-tu,\n",
       " Ivakoura,\n",
       " I,\n",
       " Nil,\n",
       " Oph,\n",
       " ville royale des Pharaons,\n",
       " Nil,\n",
       " Splendide,\n",
       " Arabie !,\n",
       " Ra,\n",
       " Nun,\n",
       " Nun !,\n",
       " I,\n",
       " Paris,\n",
       " Kama-Koura,\n",
       " trouverez,\n",
       " Yosi-Wara,\n",
       " Champ des Roseaux !,\n",
       " Yosi-Wara,\n",
       " Japon,\n",
       " Y√©do,\n",
       " D√©cid√©ment,\n",
       " Celles,\n",
       " I,\n",
       " Espagne,\n",
       " rue San-Bernardo,\n",
       " Madrid,\n",
       " Andr√®s,\n",
       " Salcedo,\n",
       " Don,\n",
       " Salcedo,\n",
       " los Rios,\n",
       " los Rios,\n",
       " Pourvu,\n",
       " alguazil,\n",
       " Telle,\n",
       " Andr√®s,\n",
       " Manille,\n",
       " Andr√®s,\n",
       " Paris,\n",
       " Souvenirs,\n",
       " Regrets,\n",
       " Petits Braconniers,\n",
       " Paris,\n",
       " I,\n",
       " France,\n",
       " Boulogne,\n",
       " Irlande,\n",
       " Ell√©nore,\n",
       " Londres,\n",
       " Boulogne,\n",
       " Paris,\n",
       " Irlande,\n",
       " Ell√©nore,\n",
       " de France,\n",
       " Paris,\n",
       " Ell√©nore,\n",
       " Ell√©nore,\n",
       " Ell√©nore,\n",
       " Versailles,\n",
       " France,\n",
       " Croixville,\n",
       " d'Italie,\n",
       " Fontainebleau,\n",
       " Seine,\n",
       " Am√©rique,\n",
       " Paris,\n",
       " Pr√©fecture,\n",
       " Prestes,\n",
       " Horizons,\n",
       " Faulhan,\n",
       " Horizons,\n",
       " soleil,\n",
       " champagne,\n",
       " France,\n",
       " Paris,\n",
       " la France,\n",
       " Paris,\n",
       " la France,\n",
       " A,\n",
       " Villeberthier,\n",
       " Quelquefois,\n",
       " I,\n",
       " Lyc√©e,\n",
       " Lorraine,\n",
       " Paris,\n",
       " Paris,\n",
       " Sais-tu,\n",
       " Mesdemoiselles,\n",
       " maman,\n",
       " Am√©rique,\n",
       " Strasbourg,\n",
       " Am√©rique,\n",
       " I,\n",
       " cath√©drale de,\n",
       " Clermont-Ferrand,\n",
       " rue de la Treille,\n",
       " Elvire,\n",
       " Janique,\n",
       " Aline,\n",
       " Auvergne,\n",
       " Paris,\n",
       " rue Saint-Jacques,\n",
       " Auvergne,\n",
       " I,\n",
       " de France,\n",
       " Oui,\n",
       " Jaff√©,\n",
       " Paris,\n",
       " Bourgogne,\n",
       " Saint-Martin-les-Mines,\n",
       " Paris,\n",
       " Batignolles,\n",
       " √Æle Saint-Louis,\n",
       " Paris,\n",
       " Bengale,\n",
       " I,\n",
       " Soudain,\n",
       " Venez,\n",
       " Ranine,\n",
       " de Russie,\n",
       " Retournez,\n",
       " I,\n",
       " Louisiane,\n",
       " R√©publique des √âtats-Unis,\n",
       " Saint-Elme,\n",
       " Nouvelle-Orl√©ans,\n",
       " Saint-Elme,\n",
       " sonnette,\n",
       " grand Cond√©,\n",
       " Krupp,\n",
       " Lebaudy,\n",
       " Saint-Elme,\n",
       " jardin des Hesp√©rides,\n",
       " Saint-Elme,\n",
       " Nouvelle-Orl√©ans,\n",
       " de France,\n",
       " I,\n",
       " Kermadec,\n",
       " P,\n",
       " Paimpol,\n",
       " Saint-Pol-de-L√©on,\n",
       " Finist√®re,\n",
       " Taille,\n",
       " Marques,\n",
       " Saint-Pol-de-L√©on,\n",
       " Finist√®re,\n",
       " la Bretagne,\n",
       " la Bretagne,\n",
       " Bugel-Du,\n",
       " Kermadec,\n",
       " h√¢le de mer,\n",
       " Saint-Pol-de-L√©on,\n",
       " Yvonne,\n",
       " I,\n",
       " √âtat,\n",
       " Port-Dieu,\n",
       " √âtat,\n",
       " Jersey,\n",
       " Port-Dieu,\n",
       " Ph√©niciens,\n",
       " Archipel,\n",
       " Manche,\n",
       " Flohy,\n",
       " Inde,\n",
       " Anglais,\n",
       " Islande,\n",
       " √âtat,\n",
       " Combien,\n",
       " Port-Dieu,\n",
       " Terre-Neuve,\n",
       " Celles,\n",
       " Prudence,\n",
       " Am√©rique,\n",
       " Pacifique,\n",
       " I,\n",
       " Chavanon,\n",
       " la France,\n",
       " Loire,\n",
       " Paris,\n",
       " Paris,\n",
       " Paris,\n",
       " Paris,\n",
       " I,\n",
       " rue de Vaugirard,\n",
       " rue Ganneron,\n",
       " H√¥tel des M√©dicis,\n",
       " Beauvais,\n",
       " Form√©,\n",
       " Paris,\n",
       " Luxembourg,\n",
       " Crozat,\n",
       " Combien,\n",
       " Noum√©a,\n",
       " Paris,\n",
       " Tiens,\n",
       " I,\n",
       " rue Notre-Dame-de-Lorette,\n",
       " Paris,\n",
       " Champs-√âlys√©es,\n",
       " bois de Boulogne,\n",
       " Venez,\n",
       " Madeleine,\n",
       " Duroy,\n",
       " Am√©ricain,\n",
       " Afrique,\n",
       " du Sud,\n",
       " Ouled-Alane,\n",
       " Sacr√©-C≈ìur,\n",
       " Hollande,\n",
       " I,\n",
       " Fran√ßais,\n",
       " Andilly,\n",
       " Ronserolles,\n",
       " Tiens,\n",
       " Cannes,\n",
       " Institut,\n",
       " Avez,\n",
       " Paris,\n",
       " bah,\n",
       " Muette,\n",
       " Passy,\n",
       " Bois de Boulogne,\n",
       " Muette,\n",
       " Derri√®re,\n",
       " Que vas-tu,\n",
       " I,\n",
       " Normandie,\n",
       " Cathelineau,\n",
       " Bocage,\n",
       " √âcosse,\n",
       " amazone,\n",
       " La Neuville,\n",
       " I,\n",
       " Napolitain,\n",
       " H√¥tel de Paris,\n",
       " Monte-Carlo,\n",
       " Voulez,\n",
       " I,\n",
       " Rue de Ch√¢teaudun,\n",
       " Bourse,\n",
       " Midi,\n",
       " de France,\n",
       " Bourse du Commerce,\n",
       " Vernier-Mareuil,\n",
       " Courbevoie,\n",
       " Bercy,\n",
       " la Seine,\n",
       " √âcole militaire,\n",
       " Vernier-Mareuil,\n",
       " √âcole,\n",
       " avenue de Tourville,\n",
       " Laboratoire,\n",
       " Vernier-Mareuil,\n",
       " Sainte-Anne,\n",
       " Vernier,\n",
       " Traduit,\n",
       " Tourville,\n",
       " Providence,\n",
       " Mareuil,\n",
       " Paris,\n",
       " Batignolles,\n",
       " Mareuil,\n",
       " Tourville,\n",
       " Tenez,\n",
       " Vernier,\n",
       " Prunelet,\n",
       " Paris,\n",
       " Avez,\n",
       " province de Normandie,\n",
       " Pyr√©n√©es,\n",
       " canton de Montserrou,\n",
       " Toulouse,\n",
       " Suzelle,\n",
       " Lapeyre,\n",
       " soleil,\n",
       " Bordelaise,\n",
       " I,\n",
       " Domfront,\n",
       " Comlie,\n",
       " Fac simile,\n",
       " I,\n",
       " Provence,\n",
       " Pi√©mont,\n",
       " d'Espagne,\n",
       " Italie,\n",
       " Saint-Pierre-de-Corbie,\n",
       " Alpes,\n",
       " du Var,\n",
       " Italie,\n",
       " de France,\n",
       " du Japon,\n",
       " Saint-Maur,\n",
       " Saint-Pierre,\n",
       " Malepeire,\n",
       " de France,\n",
       " Malepeire,\n",
       " Malepeire,\n",
       " Saint-Pierre,\n",
       " Pas de Malepeire,\n",
       " ch√¢teau de Malepeire,\n",
       " Alpes,\n",
       " Malepeire,\n",
       " Combray,\n",
       " Quelquefois,\n",
       " √àve,\n",
       " col rabattu,\n",
       " baldaquin,\n",
       " Tiens,\n",
       " Boh√™me,\n",
       " Sienne,\n",
       " Combray,\n",
       " Combray,\n",
       " Tansonville,\n",
       " lune,\n",
       " soleil,\n",
       " Combray,\n",
       " Balbec,\n",
       " Paris,\n",
       " Donci√®res,\n",
       " Venise,\n",
       " Combray,\n",
       " Brabant,\n",
       " Barbe-Bleue,\n",
       " Combray,\n",
       " Bressant,\n",
       " Combray,\n",
       " Am√©d√©e,\n",
       " Combray,\n",
       " Orl√©ans,\n",
       " √ätes,\n",
       " Combray,\n",
       " boulevard Haussmann,\n",
       " Op√©ra,\n",
       " Paris,\n",
       " Entrep√¥t,\n",
       " Lyon,\n",
       " Petite Nanette,\n",
       " Mariotte,\n",
       " Mariotte,\n",
       " Mariotte,\n",
       " Mariotte,\n",
       " Mariotte,\n",
       " fort doux,\n",
       " Suisse,\n",
       " France,\n",
       " Paris,\n",
       " Paris,\n",
       " Fran√ßais,\n",
       " Dumontet,\n",
       " Paris,\n",
       " Paris,\n",
       " Paris,\n",
       " Paris,\n",
       " Luxembourg,\n",
       " Luxembourg,\n",
       " grand bassin,\n",
       " boulevard de Gand,\n",
       " jardin du Luxembourg,\n",
       " parterre de l'Od√©on,\n",
       " vicomtesse d'Hespel,\n",
       " Paris,\n",
       " N√©lida,\n",
       " N√©lida,\n",
       " I,\n",
       " Avez,\n",
       " Bac,\n",
       " Saint-Germain,\n",
       " rue du Bac,\n",
       " Saint-Germain,\n",
       " Providence,\n",
       " I,\n",
       " Pilgrim,\n",
       " brick-go√©lette Pilgrim,\n",
       " m√©ridien de,\n",
       " San Francisco,\n",
       " Le Pilgrim,\n",
       " d√©troit de Behring,\n",
       " Tasmanie,\n",
       " cap Horn,\n",
       " oc√©an Antarctique,\n",
       " Nouvelle-Z√©lande,\n",
       " cap de Bonne-Esp√©rance,\n",
       " Pacifique,\n",
       " Atlantique,\n",
       " San Francisco,\n",
       " Pilgrim,\n",
       " Nord-caper,\n",
       " oc√©an Bor√©al,\n",
       " Sulpher-boltone,\n",
       " du Sud,\n",
       " Astrolabe,\n",
       " Z√©l√©e,\n",
       " Fran√ßais,\n",
       " le Pilgrim,\n",
       " Le Pilgrim,\n",
       " Nouvelle-Z√©lande,\n",
       " Auckland,\n",
       " golfe de Chouraki,\n",
       " Pilgrim,\n",
       " Auckland,\n",
       " Pilgrim,\n",
       " Auckland,\n",
       " Mrs,\n",
       " Pilgrim,\n",
       " Auckland,\n",
       " Nouvelle-Z√©lande,\n",
       " San Francisco,\n",
       " Auckland,\n",
       " Mrs,\n",
       " Pilgrim,\n",
       " San Francisco,\n",
       " Mrs,\n",
       " Australie,\n",
       " Melbourne,\n",
       " isthme de Panama,\n",
       " Pap√©iti,\n",
       " Panama,\n",
       " Californie,\n",
       " le Pilgrim,\n",
       " Auckland,\n",
       " San Francisco,\n",
       " Nan,\n",
       " √âquateur,\n",
       " Mrs,\n",
       " Mrs,\n",
       " Pilgrim,\n",
       " Valparaiso,\n",
       " Chili,\n",
       " Mrs,\n",
       " Le Pilgrim,\n",
       " I,\n",
       " Basse Hongrie,\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spat_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place d'une √©valuation objective\n",
    "Tr√®s important : besoin d'avoir une r√©f√©rence valid√©e, aussi appel√©e \"v√©rit√© terrain\" (*\"ground truth\"*), \"donn√©es cibles\" (*\"targets\"*), *\"gold standard\"*‚Ä¶\n",
    "\n",
    "Cette r√©f√©rence contient, pour une √©chantillon repr√©sentatif de donn√©es d'entr√©e de notre syst√®me, les donn√©es id√©ales que notre syst√®me devrait produire en sortie.\n",
    "Dans le doute, il est important de bien coller √† la d√©finition d'une t√¢che de traitement de donn√©es \"classique\", c'est √† dire √† un triplet (type et format des donn√©es d'entr√©es, type et format des donn√©es de sortie, m√©thode d'√©valuation de la conformit√© entre donn√©es pr√©dite et donn√©es attendues) commun√©ment utilis√© par les √©quipes exp√©riment√©es sur ce sujet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO introduire notions de precision/recall/fscore (m√©triques de d√©tection / retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text and target entities for 100 samples.\n"
     ]
    }
   ],
   "source": [
    "# On charge le dataset dans un format facile\n",
    "import json\n",
    "def load_dataset(path_to_json: str) -> dict[str, tuple[str, list[tuple[int, int, str]]]]:\n",
    "    with open(path_to_json, encoding=\"utf8\") as in_file:\n",
    "        return json.load(in_file)\n",
    "\n",
    "all_data = load_dataset(\"../dataset/French_ELTEC_NER_Open_Dataset.json\")\n",
    "print(f\"Loaded text and target entities for {len(all_data)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.scorer import Scorer\n",
    "from spacy.training.example import Example\n",
    "\n",
    "def evaluate(ner_model, dataset_dict, debug=False):\n",
    "    \"\"\"FIXME DOC\"\"\"\n",
    "    examples = []\n",
    "    for doc_id, (text, target_entities) in dataset_dict.items():\n",
    "        pred_doc = ner_model(text)\n",
    "        if debug:\n",
    "            print(\"Pred.:\", [(ent.text, ent.label_) for ent in pred_doc.ents], \" ‚Üî Targ.:\", [(text[e[0]:e[1]], e[2]) for e in target_entities])\n",
    "        try:\n",
    "            example = Example.from_dict(pred_doc, {\"entities\": target_entities})\n",
    "            examples.append(example)\n",
    "        except ValueError as e:\n",
    "            err_msg = f\"Error parsing document '{doc_id}': \"\n",
    "            err_msg += getattr(e, \"msg\", str(e))\n",
    "            print(err_msg)\n",
    "            raise ValueError(err_msg)\n",
    "    \n",
    "    scorer = Scorer()\n",
    "    scores = scorer.score_spans(examples, \"ents\")\n",
    "    # print(scores[\"ents_f\"])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a NER model\n",
    "ner_model = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should deactivate the useless parts of the pipeline here, to accelerate the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model.select_pipes(enable=\"ner\")\n",
    "ner_model.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"PREMI√àRE PARTIE -- LA CONSPIRATION EN DENTELLES\n",
      "O√π...\" with entities \"[[51, 58, 'PER'], [106, 113, 'PER'], [369, 374, 'L...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"LE BANQUET\n",
      "Dans la grande salle des f√™tes de 1' ¬´ ...\" with entities \"[[1003, 1014, 'PER'], [1246, 1252, 'PER'], [1254, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "\n",
      "\n",
      "\n",
      "CHAPITRE PREMIER\n",
      "PREMIERS SIGNES\n",
      "Je suis toute...\" with entities \"[[122, 140, 'PER'], [160, 168, 'PER'], [998, 1006,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.77 s, sys: 213 ms, total: 7.98 s\n",
      "Wall time: 7.98 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ents_p': 0.4317656129529684,\n",
       " 'ents_r': 0.6339622641509434,\n",
       " 'ents_f': 0.5136829231004433,\n",
       " 'ents_per_type': {'MISC': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'PER': {'p': 0.6165458937198067,\n",
       "   'r': 0.5984759671746777,\n",
       "   'f': 0.6073765615704938},\n",
       "  'LOC': {'p': 0.4205488194001276,\n",
       "   'r': 0.698093220338983,\n",
       "   'f': 0.5248904818797292},\n",
       "  'ORG': {'p': 0.0, 'r': 0.0, 'f': 0.0}}}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# evaluate using custom function, maybe useless because of the Language.evaluate() method! <https://spacy.io/api/language#evaluate>\n",
    "results = evaluate(ner_model, all_data, debug=False)\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Try the evaluation using the [`Language.evaluate()`](https://spacy.io/api/language#evaluate) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"PREMI√àRE PARTIE -- LA CONSPIRATION EN DENTELLES\n",
      "O√π...\" with entities \"[[51, 58, 'PER'], [106, 113, 'PER'], [369, 374, 'L...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"LE BANQUET\n",
      "Dans la grande salle des f√™tes de 1' ¬´ ...\" with entities \"[[1003, 1014, 'PER'], [1246, 1252, 'PER'], [1254, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "\n",
      "\n",
      "\n",
      "CHAPITRE PREMIER\n",
      "PREMIERS SIGNES\n",
      "Je suis toute...\" with entities \"[[122, 140, 'PER'], [160, 168, 'PER'], [998, 1006,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 100 examples.\n",
      "CPU times: user 3.13 s, sys: 2.09 ms, total: 3.13 s\n",
      "Wall time: 3.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "examples = []\n",
    "for doc_id, (text, target_entities) in all_data.items():\n",
    "    base_doc = ner_model.make_doc(text)  # We create simpler examples here but will the evaluate function recompute them?\n",
    "    try:\n",
    "        example = Example.from_dict(base_doc, {\"entities\": target_entities})\n",
    "        examples.append(example)\n",
    "    except ValueError as e:\n",
    "        err_msg = f\"Error parsing document '{doc_id}': \"\n",
    "        err_msg += getattr(e, \"msg\", str(e))\n",
    "        print(err_msg)\n",
    "        raise ValueError(err_msg)\n",
    "print(f\"Created {len(examples)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.78 s, sys: 8.48 s, total: 16.3 s\n",
      "Wall time: 16.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'token_acc': 1.0,\n",
       " 'token_p': 1.0,\n",
       " 'token_r': 1.0,\n",
       " 'token_f': 1.0,\n",
       " 'ents_p': 0.43243243243243246,\n",
       " 'ents_r': 0.6339622641509434,\n",
       " 'ents_f': 0.5141545524100996,\n",
       " 'ents_per_type': {'LOC': {'p': 0.4216250799744082,\n",
       "   'r': 0.698093220338983,\n",
       "   'f': 0.5257279617072198},\n",
       "  'MISC': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'PER': {'p': 0.6172914147521161,\n",
       "   'r': 0.5984759671746777,\n",
       "   'f': 0.6077380952380953},\n",
       "  'ORG': {'p': 0.0, 'r': 0.0, 'f': 0.0}},\n",
       " 'speed': 9838.436375865644}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "scores = ner_model.evaluate(examples)\n",
    "scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient les m√™mes valeurs, mais plus lentement ; probablement car on fait une √©valuation plus large avec l'√©valuation de la tokenization et de la vitesse en plus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO produire soi-m√™me des donn√©es d'entra√Ænement ou de test\n",
    "\n",
    "jeu de test : besoin d'une quantit√© et d'une vari√©t√© suffisantes pour que les r√©sulats soient significatifs. Ce jeu de donn√©es ne peut pas contenir de donn√©es vues pendant l'entra√Ænement.\n",
    "\n",
    "jeu d'entra√Ænement : g√©n√©ralement besoin d'une quantit√© plus importante pour permettre la stabilisation des param√®tres statistiques d'un mod√®le.\n",
    "Ces donn√©es doivent √™tre suffisamment vari√©es pour permettre de capturer les subtilit√©s des donn√©es √† traiter, et assez repr√©sentatives pour capturer en priorit√© les g√©n√©ralit√©s.\n",
    "\n",
    "Dans les 2 cas, il faut pr√©parer :\n",
    "- de exemples de donn√©es d'entr√©e pour le syt√®me (√©chantillons de textes)\n",
    "- les sorties parfaites attendues pour ces donn√©es (dans le cas du NER, liste des entit√©s ‚Äî avec position et √©tiquette ‚Äî √† extraire)\n",
    "\n",
    "\n",
    "TODO indiquer exemple proc√©dure la plus basique possible :\n",
    "- identifier groupe de textes √† √©tiqueter\n",
    "- les importer dans <https://tecoholic.github.io/ner-annotator/> et annoter\n",
    "- exporter les donn√©es\n",
    "- les convertir au format adapt√©\n",
    "\n",
    "### ü§ì Pour aller plus loin\n",
    "Questions √† pointer (sans forc√©ment y r√©pondre car √ßa serait pour une autre formation/atelier) :\n",
    "- quelles √©tiquettes/labels ?\n",
    "- quelles r√®gles suivre, comment g√©rer les ambigu√Øt√©s ?\n",
    "- comment distribuer le travail ? Comment assurer la coh√©rence entre le travail des diff√©rents annotateurs ?\n",
    "- Comment diffuser ton travail, le partager, quelle licence utiliser ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hn-ariane-ner-tuto-2023-5IA5eVhR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
