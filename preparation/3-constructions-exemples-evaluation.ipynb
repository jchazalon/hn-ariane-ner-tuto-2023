{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place d'une évaluation objective\n",
    "Très important : besoin d'avoir une référence validée, aussi appelée \"vérité terrain\" (*\"ground truth\"*), \"données cibles\" (*\"targets\"*), *\"gold standard\"*…\n",
    "\n",
    "Cette référence contient, pour une échantillon représentatif de données d'entrée de notre système, les données idéales que notre système devrait produire en sortie.\n",
    "Dans le doute, il est important de bien coller à la définition d'une tâche de traitement de données \"classique\", c'est à dire à un triplet (type et format des données d'entrées, type et format des données de sortie, méthode d'évaluation de la conformité entre données prédite et données attendues) communément utilisé par les équipes expérimentées sur ce sujet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO introduire notions de precision/recall/fscore (métriques de détection / retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text and target entities for 100 samples.\n"
     ]
    }
   ],
   "source": [
    "# On charge le dataset dans un format facile\n",
    "import json\n",
    "def load_dataset(path_to_json: str) -> dict[str, tuple[str, list[tuple[int, int, str]]]]:\n",
    "    with open(path_to_json, encoding=\"utf8\") as in_file:\n",
    "        return json.load(in_file)\n",
    "\n",
    "all_data = load_dataset(\"../dataset/French_ELTEC_NER_Open_Dataset.json\")\n",
    "print(f\"Loaded text and target entities for {len(all_data)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.scorer import Scorer\n",
    "from spacy.training.example import Example\n",
    "\n",
    "def evaluate(ner_model, dataset_dict, debug=False):\n",
    "    \"\"\"FIXME DOC\"\"\"\n",
    "    examples = []\n",
    "    for doc_id, (text, target_entities) in dataset_dict.items():\n",
    "        pred_doc = ner_model(text)\n",
    "        if debug:\n",
    "            print(\"Pred.:\", [(ent.text, ent.label_) for ent in pred_doc.ents], \" ↔ Targ.:\", [(text[e[0]:e[1]], e[2]) for e in target_entities])\n",
    "        try:\n",
    "            example = Example.from_dict(pred_doc, {\"entities\": target_entities})\n",
    "            examples.append(example)\n",
    "        except ValueError as e:\n",
    "            err_msg = f\"Error parsing document '{doc_id}': \"\n",
    "            err_msg += getattr(e, \"msg\", str(e))\n",
    "            print(err_msg)\n",
    "            raise ValueError(err_msg)\n",
    "    \n",
    "    scorer = Scorer()\n",
    "    scores = scorer.score_spans(examples, \"ents\")\n",
    "    # print(scores[\"ents_f\"])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a NER model\n",
    "ner_model = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should deactivate the useless parts of the pipeline here, to accelerate the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_model.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_model.select_pipes(enable=\"ner\")\n",
    "ner_model.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"PREMIÈRE PARTIE -- LA CONSPIRATION EN DENTELLES\n",
      "Où...\" with entities \"[[51, 58, 'PER'], [106, 113, 'PER'], [369, 374, 'L...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"LE BANQUET\n",
      "Dans la grande salle des fêtes de 1' « ...\" with entities \"[[1003, 1014, 'PER'], [1246, 1252, 'PER'], [1254, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "\n",
      "\n",
      "\n",
      "CHAPITRE PREMIER\n",
      "PREMIERS SIGNES\n",
      "Je suis toute...\" with entities \"[[122, 140, 'PER'], [160, 168, 'PER'], [998, 1006,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.77 s, sys: 213 ms, total: 7.98 s\n",
      "Wall time: 7.98 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ents_p': 0.4317656129529684,\n",
       " 'ents_r': 0.6339622641509434,\n",
       " 'ents_f': 0.5136829231004433,\n",
       " 'ents_per_type': {'MISC': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'PER': {'p': 0.6165458937198067,\n",
       "   'r': 0.5984759671746777,\n",
       "   'f': 0.6073765615704938},\n",
       "  'LOC': {'p': 0.4205488194001276,\n",
       "   'r': 0.698093220338983,\n",
       "   'f': 0.5248904818797292},\n",
       "  'ORG': {'p': 0.0, 'r': 0.0, 'f': 0.0}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# evaluate using custom function, maybe useless because of the Language.evaluate() method! <https://spacy.io/api/language#evaluate>\n",
    "results = evaluate(ner_model, all_data, debug=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Try the evaluation using the [`Language.evaluate()`](https://spacy.io/api/language#evaluate) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"PREMIÈRE PARTIE -- LA CONSPIRATION EN DENTELLES\n",
      "Où...\" with entities \"[[51, 58, 'PER'], [106, 113, 'PER'], [369, 374, 'L...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"LE BANQUET\n",
      "Dans la grande salle des fêtes de 1' « ...\" with entities \"[[1003, 1014, 'PER'], [1246, 1252, 'PER'], [1254, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "\n",
      "\n",
      "\n",
      "CHAPITRE PREMIER\n",
      "PREMIERS SIGNES\n",
      "Je suis toute...\" with entities \"[[122, 140, 'PER'], [160, 168, 'PER'], [998, 1006,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 100 examples.\n",
      "CPU times: user 3.13 s, sys: 2.09 ms, total: 3.13 s\n",
      "Wall time: 3.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "examples = []\n",
    "for doc_id, (text, target_entities) in all_data.items():\n",
    "    base_doc = ner_model.make_doc(text)  # We create simpler examples here but will the evaluate function recompute them?\n",
    "    try:\n",
    "        example = Example.from_dict(base_doc, {\"entities\": target_entities})\n",
    "        examples.append(example)\n",
    "    except ValueError as e:\n",
    "        err_msg = f\"Error parsing document '{doc_id}': \"\n",
    "        err_msg += getattr(e, \"msg\", str(e))\n",
    "        print(err_msg)\n",
    "        raise ValueError(err_msg)\n",
    "print(f\"Created {len(examples)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.78 s, sys: 8.48 s, total: 16.3 s\n",
      "Wall time: 16.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'token_acc': 1.0,\n",
       " 'token_p': 1.0,\n",
       " 'token_r': 1.0,\n",
       " 'token_f': 1.0,\n",
       " 'ents_p': 0.43243243243243246,\n",
       " 'ents_r': 0.6339622641509434,\n",
       " 'ents_f': 0.5141545524100996,\n",
       " 'ents_per_type': {'LOC': {'p': 0.4216250799744082,\n",
       "   'r': 0.698093220338983,\n",
       "   'f': 0.5257279617072198},\n",
       "  'MISC': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'PER': {'p': 0.6172914147521161,\n",
       "   'r': 0.5984759671746777,\n",
       "   'f': 0.6077380952380953},\n",
       "  'ORG': {'p': 0.0, 'r': 0.0, 'f': 0.0}},\n",
       " 'speed': 9838.436375865644}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "scores = ner_model.evaluate(examples)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient les mêmes valeurs, mais plus lentement ; probablement car on fait une évaluation plus large avec l'évaluation de la tokenization et de la vitesse en plus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
