{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune an [`EntityRecognizer`](https://spacy.io/api/entityrecognizer)\n",
    "Goal: get a better performance than the off-the-shelf network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a dataset with 100 documents.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def load_dataset(path_to_json: str) -> dict[str, tuple[str, list[tuple[int, int, str]]]]:\n",
    "    with open(path_to_json, encoding=\"utf8\") as in_file:\n",
    "        return json.load(in_file)\n",
    "\n",
    "dataset = load_dataset(\"../dataset/French_ELTEC_NER_Open_Dataset.json\")\n",
    "print(f\"Loaded a dataset with {len(dataset)} documents.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / val separation\n",
    "\n",
    "We will use 70 documents to train, and 30 to validate (measure) the effective performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced a training set with 70 elements, and a test set with 30 elements.\n"
     ]
    }
   ],
   "source": [
    "# All objects are list with aligned content, \n",
    "# i.e. train_ids[i] is the document ID for the ith element,\n",
    "# train_texts[i] contains the text for this particular document,\n",
    "# and train_entities[i] contains the target entities for this document.\n",
    "# test_* variants are structured the same way, with 70% of the samples in train, and 30% in test.\n",
    "train_ids = []\n",
    "train_texts = []\n",
    "train_entities = []\n",
    "test_ids = []\n",
    "test_texts = []\n",
    "test_entities = []\n",
    "for count, (id, (text, entity)) in enumerate(dataset.items()):\n",
    "    # Switch the destination based on the percentage of elements already added\n",
    "    # dict storage is shuffled but deterministic, so no need to seed any RNG here\n",
    "    dst_ids, dst_texts, dst_entities = (\n",
    "        (train_ids, train_texts, train_entities)\n",
    "        if (100 * count / len(dataset)) < 70 else\n",
    "        (test_ids, test_texts, test_entities)\n",
    "    )\n",
    "    for dst, item in zip((dst_ids, dst_texts, dst_entities), (id, text, entity)):\n",
    "        dst.append(item)\n",
    "print(f\"Produced a training set with {len(train_ids)} elements, and a test set with {len(test_ids)} elements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the performance of a pre-trained network\n",
    "\n",
    "We can compare the performance of two pre-trained network and a fine-tuned one:\n",
    "- `fr_core_news_sm`: small \n",
    "- `fr_core_news_lg`: large\n",
    "- `fr_core_news_lg+finetune`: large + fine-tuning (performed by us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.scorer import Scorer\n",
    "from spacy.training.example import Example\n",
    "\n",
    "def evaluate(ner_model, dataset_dict, debug=False):\n",
    "    \"\"\"FIXME DOC\"\"\"\n",
    "    examples = []\n",
    "    for doc_id, (text, target_entities) in dataset_dict.items():\n",
    "        pred_doc = ner_model(text)\n",
    "        if debug:\n",
    "            print(\"Pred.:\", [(ent.text, ent.label_) for ent in pred_doc.ents], \" ↔ Targ.:\", [(text[e[0]:e[1]], e[2]) for e in target_entities])\n",
    "        try:\n",
    "            example = Example.from_dict(pred_doc, {\"entities\": target_entities})\n",
    "            examples.append(example)\n",
    "        except ValueError as e:\n",
    "            err_msg = f\"Error parsing document '{doc_id}': \"\n",
    "            err_msg += getattr(e, \"msg\", str(e))\n",
    "            print(err_msg)\n",
    "            raise ValueError(err_msg)\n",
    "    \n",
    "    scorer = Scorer()\n",
    "    scores = scorer.score_spans(examples, \"ents\")\n",
    "    # print(scores[\"ents_f\"])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict = {k:dataset[k] for k in test_ids}\n",
    "len(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"LE BANQUET\n",
      "Dans la grande salle des fêtes de 1' « ...\" with entities \"[[1003, 1014, 'PER'], [1246, 1252, 'PER'], [1254, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ents_p': 0.4423305588585018,\n",
       " 'ents_r': 0.6876155268022182,\n",
       " 'ents_f': 0.5383502170767005,\n",
       " 'ents_per_type': {'LOC': {'p': 0.4674922600619195,\n",
       "   'r': 0.7259615384615384,\n",
       "   'f': 0.568738229755179},\n",
       "  'ORG': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'MISC': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'PER': {'p': 0.5846560846560847,\n",
       "   'r': 0.6636636636636637,\n",
       "   'f': 0.6216596343178622}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_model = spacy.load(\"fr_core_news_sm\")\n",
    "evaluate(nlp_model, test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-lg==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_lg-3.7.0/fr_core_news_lg-3.7.0-py3-none-any.whl (571.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.8/571.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from fr-core-news-lg==3.7.0) (3.7.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.3.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.26.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.1.3)\n",
      "Installing collected packages: fr-core-news-lg\n",
      "Successfully installed fr-core-news-lg-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jchazalo/.virtualenvs/hn-ariane-ner-tuto-2023-PwR_0BG5/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"LE BANQUET\n",
      "Dans la grande salle des fêtes de 1' « ...\" with entities \"[[1003, 1014, 'PER'], [1246, 1252, 'PER'], [1254, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ents_p': 0.48466257668711654,\n",
       " 'ents_r': 0.7301293900184843,\n",
       " 'ents_f': 0.5825958702064897,\n",
       " 'ents_per_type': {'LOC': {'p': 0.6037735849056604,\n",
       "   'r': 0.7692307692307693,\n",
       "   'f': 0.6765327695560254},\n",
       "  'ORG': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'MISC': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'PER': {'p': 0.649171270718232,\n",
       "   'r': 0.7057057057057057,\n",
       "   'f': 0.6762589928057553}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -m spacy download fr_core_news_lg\n",
    "nlp_model = spacy.load(\"fr_core_news_lg\")\n",
    "evaluate(nlp_model, test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit ici que la performance est meilleure pour le modèle large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert training data to Spacy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# adapted from <https://spacy.io/usage/training#training-data>\n",
    "def prepare_save_dataset(ids, texts, entities, dst_path):\n",
    "    nlp = spacy.blank(\"fr\")\n",
    "    # the DocBin will store the example documents\n",
    "    db = DocBin()\n",
    "    for id, text, annotations in zip(ids, texts, entities):\n",
    "        ents = None\n",
    "        doc = nlp(text)\n",
    "        ents = []\n",
    "        for start, end, label in annotations:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            base_err_str = None\n",
    "            if span is None:\n",
    "                print(f\"Warning: in document '{id}', could align entity '{text[start:end]}' to computed tokens. \")\n",
    "                span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "                print(f\"\\t Try to fix by contracting to span '{span}'.\")\n",
    "            if span is None:\n",
    "                span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
    "                print(f\"\\t Try to fix by expanding to span '{span}'.\")\n",
    "            if span is None:\n",
    "                print(f\"\\tCannot recover: skipping.\")\n",
    "                continue\n",
    "            ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    db.to_disk(dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare training set…\n",
      "Warning: in document 'FRA01201_Feval', could align entity 'roi Philippe V' to computed tokens. \n",
      "\t Try to fix by contracting to span 'roi Philippe'.\n",
      "Prepare test set…\n",
      "Warning: in document 'FRA02001_Gilbert', could align entity 'Bordeaux' to computed tokens. \n",
      "\t Try to fix by contracting to span 'None'.\n",
      "\t Try to fix by expanding to span 'Bordeaux-'.\n"
     ]
    }
   ],
   "source": [
    "print(\"Prepare training set…\")\n",
    "prepare_save_dataset(train_ids, train_texts, train_entities, \"../tmp/train.spacy\")\n",
    "print(\"Prepare test set…\")\n",
    "prepare_save_dataset(test_ids, test_texts, test_entities, \"../tmp/test.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "def display_doc(id, ids, text, entities):\n",
    "    ii = ids.index(id)\n",
    "    from spacy import displacy\n",
    "    manual_content = {\n",
    "        \"text\": text[ii],\n",
    "        \"ents\": [{\"start\": e[0], \"end\": e[1], \"label\": e[2]} for e in entities[ii]],\n",
    "        \"title\": id\n",
    "    }\n",
    "    displacy.render(manual_content, manual=True, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_doc(\"FRA01002_DelarueMardrus\", test_ids, test_texts, test_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training scripts and launch the training\n",
    "\n",
    "TODO use <https://spacy.io/usage/training#quickstart>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essai 1 : training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo '\n",
    "# This is an auto-generated partial config. To use it with 'spacy train'\n",
    "# you can run spacy init fill-config to auto-fill all default settings:\n",
    "# python -m spacy init fill-config ./base_config.cfg ./config.cfg\n",
    "[paths]\n",
    "train = null\n",
    "dev = null\n",
    "vectors = null\n",
    "[system]\n",
    "gpu_allocator = null\n",
    "\n",
    "[nlp]\n",
    "lang = \"fr\"\n",
    "pipeline = [\"tok2vec\",\"ner\"]\n",
    "batch_size = 1000\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.tok2vec]\n",
    "factory = \"tok2vec\"\n",
    "\n",
    "[components.tok2vec.model]\n",
    "@architectures = \"spacy.Tok2Vec.v2\"\n",
    "\n",
    "[components.tok2vec.model.embed]\n",
    "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
    "width = ${components.tok2vec.model.encode.width}\n",
    "attrs = [\"NORM\", \"PREFIX\", \"SUFFIX\", \"SHAPE\"]\n",
    "rows = [5000, 1000, 2500, 2500]\n",
    "include_static_vectors = false\n",
    "\n",
    "[components.tok2vec.model.encode]\n",
    "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
    "width = 96\n",
    "depth = 4\n",
    "window_size = 1\n",
    "maxout_pieces = 3\n",
    "\n",
    "[components.ner]\n",
    "factory = \"ner\"\n",
    "\n",
    "[components.ner.model]\n",
    "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
    "state_type = \"ner\"\n",
    "extra_state_tokens = false\n",
    "hidden_width = 64\n",
    "maxout_pieces = 2\n",
    "use_upper = true\n",
    "nO = null\n",
    "\n",
    "[components.ner.model.tok2vec]\n",
    "@architectures = \"spacy.Tok2VecListener.v1\"\n",
    "width = ${components.tok2vec.model.encode.width}\n",
    "\n",
    "[corpora]\n",
    "\n",
    "[corpora.train]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "path = ${paths.train}\n",
    "max_length = 0\n",
    "\n",
    "[corpora.dev]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "path = ${paths.dev}\n",
    "max_length = 0\n",
    "\n",
    "[training]\n",
    "dev_corpus = \"corpora.dev\"\n",
    "train_corpus = \"corpora.train\"\n",
    "\n",
    "[training.optimizer]\n",
    "@optimizers = \"Adam.v1\"\n",
    "\n",
    "[training.batcher]\n",
    "@batchers = \"spacy.batch_by_words.v1\"\n",
    "discard_oversize = false\n",
    "tolerance = 0.2\n",
    "\n",
    "[training.batcher.size]\n",
    "@schedules = \"compounding.v1\"\n",
    "start = 100\n",
    "stop = 1000\n",
    "compound = 1.001\n",
    "\n",
    "[initialize]\n",
    "vectors = ${paths.vectors}\n",
    "' > ../tmp/base_config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "../tmp/config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config ../tmp/base_config.cfg ../tmp/config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: ../tmp/output\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: ../tmp/output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    273.60    0.83    0.43   14.94    0.01\n",
      "  2     200        935.92  14557.13   58.94   60.78   57.20    0.59\n",
      "  5     400        396.65   2381.73   63.99   66.34   61.81    0.64\n",
      "  8     600        438.42   1145.71   58.71   63.71   54.43    0.59\n",
      " 11     800        660.69   1098.65   64.33   68.18   60.89    0.64\n",
      " 14    1000        584.34    701.06   59.94   65.36   55.35    0.60\n",
      " 17    1200        504.11    474.06   63.68   67.42   60.33    0.64\n",
      " 20    1400        525.60    436.09   63.35   64.56   62.18    0.63\n",
      " 22    1600        529.14    317.35   62.56   67.74   58.12    0.63\n",
      " 25    1800        553.02    292.07   62.50   65.26   59.96    0.63\n",
      " 28    2000        492.68    288.30   59.04   60.00   58.12    0.59\n",
      " 31    2200        596.61    259.85   59.75   61.08   58.49    0.60\n",
      " 34    2400        557.34    272.29   65.16   69.60   61.25    0.65\n",
      " 37    2600        496.00    180.26   60.89   63.71   58.30    0.61\n",
      " 40    2800        410.16    147.70   61.40   69.21   55.17    0.61\n",
      " 42    3000        607.81    232.52   61.83   67.03   57.38    0.62\n",
      " 45    3200        324.12    128.61   63.40   66.13   60.89    0.63\n",
      " 48    3400        445.81    117.01   57.97   63.52   53.32    0.58\n",
      " 51    3600        561.37    161.01   60.18   63.71   57.01    0.60\n",
      " 54    3800        796.32    220.69   58.92   65.83   53.32    0.59\n",
      " 57    4000        619.28    186.40   61.05   65.95   56.83    0.61\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../tmp/output/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ../tmp/config.cfg --output ../tmp/output --paths.train ../tmp/train.spacy --paths.dev ../tmp/test.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement du meilleur modèle et utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Georges PER\n",
      "Paul PER\n",
      "Paris Du Rozel LOC\n",
      "Allemagne LOC\n",
      "Paris LOC\n",
      "Normandie LOC\n",
      "Colin PER\n",
      "l'abbaye du Rozel LOC\n",
      "abbaye du Rozel LOC\n",
      "madame V… PER\n",
      "abbaye du Rozel LOC\n",
      "Rozel LOC\n",
      "l'abbaye LOC\n",
      "Cérès PER\n",
      "Malade LOC\n",
      "Aventures de Télémaque LOC\n",
      "Mentor PER\n",
      "Robinson PER\n",
      "Cologne LOC\n",
      "Paris LOC\n",
      "Iliade LOC\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"../tmp/output/model-best\")\n",
    "some_text = test_texts[0]\n",
    "doc = nlp(some_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est même possible de créer un package Python facile à partager, sauvegarder et réutiliser avec la commande [`spacy package`](https://spacy.io/api/cli#package)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Et pour aller plus loin : fine-tuner un modèle large\n",
    "\n",
    "Repartir de l'assistant de configuration pour modifier les paramètres suivants :\n",
    "- sélectionner un modèle optimisé pour la performance (qui pré-sélectionne le modèle `fr_core_news_lg`)\n",
    "- sélectionner l'utilisation d'accélérateur GPU\n",
    "\n",
    "Relancer l'entræînement sur une machine avec GPU (type Google Colab, en demandant une runtime GPU \"T4\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hn-ariane-ner-tuto-2023-PwR_0BG5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
