{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse 1 : utilisation de regexp et de patterns spacy\n",
    "\n",
    "on essaie d'extraire des termes connus, et des locutions typiques, pour détecter les références aux personnages et aux lieux.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trouver et lister les fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TEXT_FILES_DIR = \"/home/joseph/tmp/French_ELTEC_NER_Open_Dataset/texts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/joseph/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00101_Adam.txt',\n",
       " '/home/joseph/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00102_Adam.txt',\n",
       " '/home/joseph/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00201_Audoux.txt',\n",
       " '/home/joseph/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00301_Aimard.txt',\n",
       " '/home/joseph/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00302_Aimard.txt',\n",
       " '/home/joseph/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00401_Allais.txt',\n",
       " '/home/joseph/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00501_Balzac.txt',\n",
       " '/home/joseph/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00502_Balzac.txt',\n",
       " '/home/joseph/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00503_Balzac.txt',\n",
       " '/home/joseph/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA00601_Boisgobey.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = sorted(glob(os.path.join(PATH_TEXT_FILES_DIR, \"*.txt\")))\n",
    "print(\"Found\", len(files), \"files.\")\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joseph/tmp/French_ELTEC_NER_Open_Dataset/texts/tr_FRA03001_Ohnet.txt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = files[50]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import spacy and start processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## L'objet `nlp`\n",
    "On construit une nouvelle chaîne de traitements de plusieurs façon. La manière la plus simple est de construire une chaîne de traitement vide (ou presque) pour le français à l'aide de la \"fabrique\" à chaînes de traitement `spacy.blank(LANGAGE)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La chaîne de traitements contient différents traitements appliqués les uns après les autres.\n",
    "On peut afficher cette liste de traitements à l'aide de l'attribut `pipe_names` de l'objet `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, une chaîne de traitement ne contient rien… Sauf un *tokenizer*, d'où l'importance de préciser la langue !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'objet `doc`\n",
    "On obtient un objet `doc` en appliquant la chaîne de traitement `nlp` à une chaîne de texte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créé en traitant une chaine de caractères avec l'objet nlp\n",
    "doc = nlp(\"Bonjour tout le monde !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet objet `doc` est central pour Spacy : il contient toute les informations produite par la chaîne de traitement à propos de notre texte.\n",
    "\n",
    "On peut parcourir les *tokens* extraits à l'aide d'une boucle classique en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour\n",
      "tout\n",
      "le\n",
      "monde\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Itère sur les tokens dans un Doc\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsqu'on souhaite traiter plusieurs documents, on peut utiliser `nlp.pipe(LISTE_DE_TEXTES)`.\n",
    "Dans ce cas, on obtient une liste de documents en sortie, qu'il est possible d'inspecter avec une seconde boucle !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc#0, tok#0: Bonjour\n",
      "doc#0, tok#1: tout\n",
      "doc#0, tok#2: le\n",
      "doc#0, tok#3: monde\n",
      "doc#0, tok#4: !\n",
      "doc#1, tok#0: Comment\n",
      "doc#1, tok#1: allez\n",
      "doc#1, tok#2: -vous\n",
      "doc#1, tok#3: ?\n",
      "doc#2, tok#0: Savez\n",
      "doc#2, tok#1: -vous\n",
      "doc#2, tok#2: qu'\n",
      "doc#2, tok#3: une\n",
      "doc#2, tok#4: chaîne\n",
      "doc#2, tok#5: de\n",
      "doc#2, tok#6: caractères\n",
      "doc#2, tok#7: peut\n",
      "doc#2, tok#8: contenir\n",
      "doc#2, tok#9: des\n",
      "doc#2, tok#10: retours\n",
      "doc#2, tok#11: à\n",
      "doc#2, tok#12: la\n",
      "doc#2, tok#13: ligne\n",
      "doc#2, tok#14: \n",
      "\n",
      "doc#2, tok#15: comme\n",
      "doc#2, tok#16: celui-ci\n",
      "doc#2, tok#17: ?\n"
     ]
    }
   ],
   "source": [
    "TEXTES = [\n",
    "    \"Bonjour tout le monde !\", \n",
    "    \"Comment allez-vous ?\",\n",
    "    \"Savez-vous qu'une chaîne de caractères peut contenir des retours à la ligne\\ncomme celui-ci ?\"\n",
    "    ]\n",
    "documents = nlp.pipe(TEXTES)\n",
    "for doc_id, doc in enumerate(documents):\n",
    "    for token_id, token in enumerate(doc):\n",
    "        print(f\"doc#{doc_id}, tok#{token_id}: {token.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hn-ariane-ner-tuto-2023-5IA5eVhR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
